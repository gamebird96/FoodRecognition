# -*- coding: utf-8 -*-
"""model_prev_light.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hoihTt9J4OEAAwrbvnUv2Dj9hFs3IYej
"""

##Imports
import matplotlib.pyplot as plt
import cv2




import os
import keras
import numpy as np
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D
from keras.layers import Activation, Dropout, Flatten, Dense
from keras.optimizers import SGD
from keras.callbacks import ModelCheckpoint, History
from keras import backend as K
from keras.preprocessing.image import ImageDataGenerator



###This Code was used to split the dataset
'''
#Split images into training and testing parts
#Note: run only one time!!
def splitImageSet(rootFolder, outFolder, p):
    import os
    from os import listdir
    import numpy as np
    from PIL import Image
    cats = listdir(rootFolder) # a list of subfolder names
    for cat in cats:
        print 'Image Category...{}'.format(cat)
        
        folderPath = (os.path.join(rootFolder,cat))
        imgNames = listdir(folderPath)
        imgPaths = [os.path.join(folderPath,imgName) for imgName in imgNames]
        idx = np.random.permutation(len(imgPaths))
        trainIdx = idx[:int(p*len(idx))]
        testIdx = [ind for ind in idx if not ind in trainIdx]
        
        if not os.path.exists(os.path.join(outFolder,'Train',cat)):
            os.makedirs(os.path.join(outFolder,'Train',cat))
        for k in range(len(trainIdx)):
            img = Image.open(os.path.join(imgPaths[trainIdx[k]]))
            #temp = os.path.join(outFolder,'train',cat,imgNames[trainIdx[k]])
            img.save(os.path.join(outFolder,'Train',cat,imgNames[trainIdx[k]]))
        if not os.path.exists(os.path.join(outFolder,'Test',cat)):
            os.makedirs(os.path.join(outFolder,'Test',cat))
for k in range(len(testIdx)):
    img = Image.open(os.path.join(imgPaths[testIdx[k]]))
    img.save(os.path.join(outFolder,'Test',cat,imgNames[testIdx[k]]))
    
    print 'Split Done!'
    return
rootFolder = 'rootFolder' #add the image directory
outFolder = 'outFolder' #image output directory
splitImageSet(rootFolder, outFolder, 0.80)
'''

## The Actual model used in the training phase
'''Build a Sequential model'''
model = Sequential()

'''
    The first layer has 32 features uses a kernel of size 7x7, and rectified linear unit activation
'''
model.add(Conv2D(32, (7, 7), input_shape=(128, 128, 3), activation='relu'))
'''
        It is followed by a Max-Pool Layer
'''
model.add(MaxPooling2D(pool_size=(2, 2)))

##Similar layers are repeated with a kernal of 5x5 size and a max-pool
model.add(Conv2D(64, (5, 5), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

##3rd convolution layer
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

##4th convolution layer
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

## A dropout of probability 25% is applied
model.add(Dropout(0.25))

## The conv layer is flattened
model.add(Flatten())

## Two hidden layers of 128 and 64 units are used next
model.add(Dense(128))
model.add(Activation('relu'))
model.add(Dense(64))
model.add(Activation('relu'))

## A dropout of 50% was used as regularization and prevent overfitting
model.add(Dropout(0.5))

## The output class consists of 16 units activated by a softmax function
model.add(Dense(16))
model.add(Activation('softmax'))

'''
    Code taken from https://keras.io/optimizers/
'''
## Stochastic gradient descent optimized with momentum
sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
#print(model.summary())


'''
    Referred Base Code
'''

class decaylr_loss(keras.callbacks.Callback):
    def __init__(self):
        super(decaylr_loss, self).__init__()
    def on_epoch_end(self,epoch,logs={}):
        loss=logs.get('loss')
        print ("loss: ",loss)
        old_lr = 0.001
        new_lr= old_lr*np.exp(loss)
        print ("New learning rate: ", new_lr)
        K.set_value(self.model.optimizer.lr, new_lr)
lrate = decaylr_loss()

'''Code taken from https://keras.io/preprocessing/image/'''

##Generates random transformations to the input data
train_gen = ImageDataGenerator(
                                   rescale=1./255,
                                   rotation_range=20,
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True)

trainDir = 'Dataset/MergedData/Train/'
train_generator = train_gen.flow_from_directory(trainDir,
                                                    target_size=(128,128),
                                                    batch_size=32,
                                                    class_mode='categorical')

test_gen = ImageDataGenerator(rescale=1./255)
testDir = 'Dataset/MergedData/Test/'
test_generator = test_gen.flow_from_directory(testDir,
                                                  target_size=(128,128),
                                                  batch_size=32,
                                                  shuffle=False,
                                                  class_mode='categorical')

##Save weights after every 25 epochs
##https://stackoverflow.com/questions/51186330/save-model-weights-at-the-end-of-every-n-epochs
mc = keras.callbacks.ModelCheckpoint('weights{epoch:08d}.h5', 
                                     save_weights_only=True, period=25)


###Cmpare
epochs = 2
test_samples = 6067
val_samples = 1617

#Fit the model
hist = History()
model.fit_generator(train_generator,
                    samples_per_epoch= test_samples,
                    nb_epoch=epochs,
                    verbose=1,
                    validation_data=test_generator,
                    nb_val_samples=val_samples, 
                    callbacks = [lrate, hist, mc])

#evaluate the model
#eval = model.evaluate_generator(test_generator, val_samples=val_samples)
#print("Accuracy = ", eval[1])


#Generate Graphs
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.plot(hist.history['val_loss'], label='Test Data')
plt.plot(hist.history['loss'], label='Training Data')
plt.legend()
plt.savefig('Accuracy Loss.png')



'''Generate confusion matrix done manually'''

generator = test_gen.flow_from_directory(
        'Dataset/prediction_generator/Apple',
        target_size=(128, 128),
        batch_size=1,
        class_mode=None,  # only data, no labels
        shuffle=False)  # keep data in same order as labels

probabilities = model.predict_generator(generator, 210)

l=[i.argmax() for i in probabilities]

m=dict()
for i in range(16):
  m[i]=0

for i in l:
    m[i]+=1
print(m)


