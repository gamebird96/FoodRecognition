{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L29tWNazzbJ_",
    "outputId": "be8ed752-845c-47ef-c9b1-ca04150d1fec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "C6kWpN1Jz6br",
    "outputId": "e4dd666d-b6da-4b59-c598-58ef1bcd0284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc.png\t      outFolder.tar.gz\t   weights00000075.h5\n",
      "'Accuracy Loss.png'   sample_data\t   weights00000099.h5\n",
      " e200_m2_acc.png      weights00000024.h5   weights00000100.h5\n",
      " e200_m2.png\t      weights00000025.h5   weights00000124.h5\n",
      " newData\t      weights00000049.h5   weights00000149.h5\n",
      " newData.tar.gz       weights00000050.h5   weights00000174.h5\n",
      " outFolder\t      weights00000074.h5   weights00000199.h5\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "COqU48ICz9TI",
    "outputId": "3772e394-22c4-464e-b0b7-b70b80593944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: rar: command not found\n"
     ]
    }
   ],
   "source": [
    "!rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfF7_o-c0HpU"
   },
   "outputs": [],
   "source": [
    "!tar -xzf newData.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4r3G2NOYwYKp"
   },
   "source": [
    "Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7n9E1EOrwoGK"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWeh3KqUWBhm"
   },
   "source": [
    "Test for Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHWxy_331Hv9"
   },
   "outputs": [],
   "source": [
    "im=cv2.imread('Test/Apple/1011.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tpmZoIwuWOst"
   },
   "source": [
    "More Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "9LlhBJte1ahs",
    "outputId": "bed899b3-76de-469f-938c-a2f57343a3f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KngpH2U-1_-8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, History\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "U4q_1huX2ECC",
    "outputId": "b58fc5f6-15d2-4801-fd52-d452bf46866c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = Sequential()\\nmodel.add(Conv2D(32, (7, 7), input_shape=(128, 128, 3)))\\nmodel.add(Activation('relu'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Conv2D(64, (5, 5)))\\nmodel.add(Activation('relu'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Conv2D(128, (3, 3)))\\nmodel.add(Activation('relu'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Dropout(0.25))\\nmodel.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\\nmodel.add(Dense(128))\\nmodel.add(Activation('relu'))\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(10))\\nmodel.add(Activation('softmax'))\\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) #initial lr = 0.01\\nmodel.compile(loss='categorical_crossentropy',\\n              optimizer=sgd,\\n              metrics=['accuracy'])\\nprint(model.summary())\\n\""
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (7, 7), input_shape=(128, 128, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) #initial lr = 0.01\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pe2wcuEnWS3z"
   },
   "source": [
    "Our Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "1PDMFA2AAPjL",
    "outputId": "47681fe9-0c89-4c86-93ec-2cfdb22b542b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 122, 122, 32)      4736      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 61, 61, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 57, 57, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 26, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 11, 11, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               409728    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 16)                0         \n",
      "=================================================================\n",
      "Total params: 696,464\n",
      "Trainable params: 696,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (7, 7), input_shape=(128, 128, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('softmax'))\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OEp3k97cvm1M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzVApxO82F4t"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "class decaylr_loss(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super(decaylr_loss, self).__init__()\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        #loss=logs.items()[1][1] #get loss\n",
    "        loss=logs.get('loss')\n",
    "        print (\"loss: \",loss)\n",
    "        old_lr = 0.001 #needs some adjustments\n",
    "        new_lr= old_lr*np.exp(loss) #lr*exp(loss)\n",
    "        print (\"New learning rate: \", new_lr)\n",
    "        K.set_value(self.model.optimizer.lr, new_lr)\n",
    "lrate = decaylr_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1Y_6XCZsWBs"
   },
   "outputs": [],
   "source": [
    "#https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IBcD3lIg2KFE"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "                                   rescale=1./255,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WMnNbOnc2L8U",
    "outputId": "79ba6a42-032a-4051-ad04-fc1d069d7196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6067 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "trainDir = 'Dataset/MergedData/Train/'\n",
    "train_generator = train_datagen.flow_from_directory(trainDir,  \n",
    "                                                    target_size=(128,128),\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qiLShXqE2Qu_",
    "outputId": "b84ad7c7-99f3-40f4-e3c5-f3a214106117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1617 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  \n",
    "testDir = 'Dataset/MergedData/Test/'\n",
    "test_generator = test_datagen.flow_from_directory(testDir,\n",
    "                                                  target_size=(128,128),\n",
    "                                                  batch_size=32,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZn77gUADTzp"
   },
   "outputs": [],
   "source": [
    "##https://stackoverflow.com/questions/51186330/save-model-weights-at-the-end-of-every-n-epochs\n",
    "mc = keras.callbacks.ModelCheckpoint('weights{epoch:08d}.h5', \n",
    "                                     save_weights_only=True, period=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSYBezx0v4CZ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwPKD0UBv3_R"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7SPsRziv38Z"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cS135ux82kXp",
    "outputId": "ff26b11e-9663-4aba-8455-9cfb162cdb4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., verbose=1, validation_data=<keras_pre..., callbacks=[<__main__..., steps_per_epoch=189, epochs=200, validation_steps=1617)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "189/189 [==============================] - 72s 382ms/step - loss: 2.3650 - acc: 0.2390 - val_loss: 2.0274 - val_acc: 0.3316\n",
      "loss:  2.3646101871797045\n",
      "New learning rate:  0.010639890443316473\n",
      "Epoch 2/200\n",
      "189/189 [==============================] - 73s 385ms/step - loss: 2.0352 - acc: 0.3514 - val_loss: 1.9147 - val_acc: 0.3550\n",
      "loss:  2.0356800662651473\n",
      "New learning rate:  0.007657457933948305\n",
      "Epoch 3/200\n",
      "189/189 [==============================] - 72s 380ms/step - loss: 1.8383 - acc: 0.4193 - val_loss: 1.6568 - val_acc: 0.4354\n",
      "loss:  1.8383050817641327\n",
      "New learning rate:  0.0062858751825809545\n",
      "Epoch 4/200\n",
      "189/189 [==============================] - 72s 380ms/step - loss: 1.7176 - acc: 0.4467 - val_loss: 1.6098 - val_acc: 0.4328\n",
      "loss:  1.717627190101611\n",
      "New learning rate:  0.005571293148512129\n",
      "Epoch 5/200\n",
      "189/189 [==============================] - 72s 381ms/step - loss: 1.6189 - acc: 0.4833 - val_loss: 1.4851 - val_acc: 0.5065\n",
      "loss:  1.6201788950084948\n",
      "New learning rate:  0.005053994370061892\n",
      "Epoch 6/200\n",
      "189/189 [==============================] - 72s 381ms/step - loss: 1.5720 - acc: 0.4957 - val_loss: 1.4118 - val_acc: 0.5318\n",
      "loss:  1.5719446962098989\n",
      "New learning rate:  0.004816004759189824\n",
      "Epoch 7/200\n",
      "189/189 [==============================] - 72s 380ms/step - loss: 1.5078 - acc: 0.5144 - val_loss: 1.3791 - val_acc: 0.5279\n",
      "loss:  1.5076086629987653\n",
      "New learning rate:  0.004515918788198685\n",
      "Epoch 8/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 1.4722 - acc: 0.5247 - val_loss: 1.3430 - val_acc: 0.5526\n",
      "loss:  1.472284020879788\n",
      "New learning rate:  0.00435918023801716\n",
      "Epoch 9/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 1.4198 - acc: 0.5447 - val_loss: 1.3189 - val_acc: 0.5637\n",
      "loss:  1.4191286915081438\n",
      "New learning rate:  0.0041335173020303866\n",
      "Epoch 10/200\n",
      "189/189 [==============================] - 72s 381ms/step - loss: 1.3680 - acc: 0.5658 - val_loss: 1.2342 - val_acc: 0.5858\n",
      "loss:  1.3683340833714508\n",
      "New learning rate:  0.003928800187123063\n",
      "Epoch 11/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 1.3163 - acc: 0.5812 - val_loss: 1.1839 - val_acc: 0.6009\n",
      "loss:  1.315127901292183\n",
      "New learning rate:  0.0037252274161827066\n",
      "Epoch 12/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 1.2673 - acc: 0.5964 - val_loss: 1.1857 - val_acc: 0.5843\n",
      "loss:  1.26622060380302\n",
      "New learning rate:  0.0035474200890315785\n",
      "Epoch 13/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 1.2498 - acc: 0.6090 - val_loss: 1.1214 - val_acc: 0.6103\n",
      "loss:  1.2504033706458817\n",
      "New learning rate:  0.0034917511433462723\n",
      "Epoch 14/200\n",
      "189/189 [==============================] - 72s 378ms/step - loss: 1.2044 - acc: 0.6129 - val_loss: 1.1078 - val_acc: 0.6207\n",
      "loss:  1.204596950519944\n",
      "New learning rate:  0.0033354144700459124\n",
      "Epoch 15/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 1.1790 - acc: 0.6276 - val_loss: 1.0760 - val_acc: 0.6244\n",
      "loss:  1.178772163608396\n",
      "New learning rate:  0.0032503808159260825\n",
      "Epoch 16/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 1.1502 - acc: 0.6332 - val_loss: 1.0909 - val_acc: 0.6230\n",
      "loss:  1.149670920336454\n",
      "New learning rate:  0.00315715378361664\n",
      "Epoch 17/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 1.1196 - acc: 0.6431 - val_loss: 1.0046 - val_acc: 0.6645\n",
      "loss:  1.119522008425753\n",
      "New learning rate:  0.003063389578874484\n",
      "Epoch 18/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 1.1095 - acc: 0.6503 - val_loss: 0.9810 - val_acc: 0.6768\n",
      "loss:  1.1097299112993129\n",
      "New learning rate:  0.0030335389591845856\n",
      "Epoch 19/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 1.0871 - acc: 0.6574 - val_loss: 0.9813 - val_acc: 0.6656\n",
      "loss:  1.0869674831357197\n",
      "New learning rate:  0.002965268198413264\n",
      "Epoch 20/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 1.0527 - acc: 0.6663 - val_loss: 0.9569 - val_acc: 0.6757\n",
      "loss:  1.052755369918842\n",
      "New learning rate:  0.002865535861696678\n",
      "Epoch 21/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 1.0677 - acc: 0.6694 - val_loss: 0.9114 - val_acc: 0.6775\n",
      "loss:  1.0683466788649856\n",
      "New learning rate:  0.002910563423790421\n",
      "Epoch 22/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 1.0343 - acc: 0.6648 - val_loss: 0.9385 - val_acc: 0.6732\n",
      "loss:  1.0342968257646712\n",
      "New learning rate:  0.0028131274212566354\n",
      "Epoch 23/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 1.0288 - acc: 0.6670 - val_loss: 0.8968 - val_acc: 0.6963\n",
      "loss:  1.0290601678521718\n",
      "New learning rate:  0.002798434539660961\n",
      "Epoch 24/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 1.0008 - acc: 0.6831 - val_loss: 0.9067 - val_acc: 0.6964\n",
      "loss:  1.0003791004147724\n",
      "New learning rate:  0.0027193125255841923\n",
      "Epoch 25/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.9855 - acc: 0.6894 - val_loss: 0.8747 - val_acc: 0.6926\n",
      "loss:  0.9854697703327536\n",
      "New learning rate:  0.0026790701365226603\n",
      "Epoch 26/200\n",
      "189/189 [==============================] - 70s 368ms/step - loss: 0.9867 - acc: 0.6889 - val_loss: 0.8481 - val_acc: 0.6930\n",
      "loss:  0.9864601743448442\n",
      "New learning rate:  0.0026817248127185243\n",
      "Epoch 27/200\n",
      "189/189 [==============================] - 70s 372ms/step - loss: 0.9653 - acc: 0.6953 - val_loss: 0.8766 - val_acc: 0.7032\n",
      "loss:  0.9655591341021047\n",
      "New learning rate:  0.002626255675135496\n",
      "Epoch 28/200\n",
      "189/189 [==============================] - 68s 361ms/step - loss: 0.9509 - acc: 0.6934 - val_loss: 0.8752 - val_acc: 0.6892\n",
      "loss:  0.9509016734070292\n",
      "New learning rate:  0.002588042176380223\n",
      "Epoch 29/200\n",
      "189/189 [==============================] - 69s 363ms/step - loss: 0.9378 - acc: 0.7041 - val_loss: 0.8428 - val_acc: 0.7079\n",
      "loss:  0.9382463913864604\n",
      "New learning rate:  0.0025554961467184577\n",
      "Epoch 30/200\n",
      "189/189 [==============================] - 70s 370ms/step - loss: 0.9393 - acc: 0.7047 - val_loss: 0.8482 - val_acc: 0.7053\n",
      "loss:  0.9404407824972195\n",
      "New learning rate:  0.002561110062056605\n",
      "Epoch 31/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.8966 - acc: 0.7177 - val_loss: 0.8222 - val_acc: 0.7147\n",
      "loss:  0.8968483381263461\n",
      "New learning rate:  0.0024518634765692826\n",
      "Epoch 32/200\n",
      "189/189 [==============================] - 74s 391ms/step - loss: 0.9054 - acc: 0.7137 - val_loss: 0.9111 - val_acc: 0.6820\n",
      "loss:  0.9046356513861693\n",
      "New learning rate:  0.00247103144214227\n",
      "Epoch 33/200\n",
      "189/189 [==============================] - 72s 380ms/step - loss: 0.9061 - acc: 0.7055 - val_loss: 0.8396 - val_acc: 0.7192\n",
      "loss:  0.905470471235969\n",
      "New learning rate:  0.0024730951695400927\n",
      "Epoch 34/200\n",
      "189/189 [==============================] - 72s 381ms/step - loss: 0.8854 - acc: 0.7170 - val_loss: 0.8116 - val_acc: 0.7149\n",
      "loss:  0.8850458009833626\n",
      "New learning rate:  0.002423095369094775\n",
      "Epoch 35/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.8772 - acc: 0.7229 - val_loss: 0.8695 - val_acc: 0.7017\n",
      "loss:  0.8772051061587359\n",
      "New learning rate:  0.002404170905198512\n",
      "Epoch 36/200\n",
      "189/189 [==============================] - 72s 381ms/step - loss: 0.8629 - acc: 0.7230 - val_loss: 0.8550 - val_acc: 0.6928\n",
      "loss:  0.8626209992936424\n",
      "New learning rate:  0.0023693626603394067\n",
      "Epoch 37/200\n",
      "189/189 [==============================] - 72s 378ms/step - loss: 0.8403 - acc: 0.7326 - val_loss: 0.7959 - val_acc: 0.7222\n",
      "loss:  0.8392864882106512\n",
      "New learning rate:  0.00231471481112463\n",
      "Epoch 38/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.8505 - acc: 0.7362 - val_loss: 0.7533 - val_acc: 0.7204\n",
      "loss:  0.8507612934760447\n",
      "New learning rate:  0.0023414286879746326\n",
      "Epoch 39/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.8367 - acc: 0.7304 - val_loss: 0.8100 - val_acc: 0.7096\n",
      "loss:  0.8361054792419977\n",
      "New learning rate:  0.002307363381231759\n",
      "Epoch 40/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.8308 - acc: 0.7310 - val_loss: 0.7511 - val_acc: 0.7364\n",
      "loss:  0.8312920206918535\n",
      "New learning rate:  0.0022962836704927716\n",
      "Epoch 41/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.8405 - acc: 0.7360 - val_loss: 0.8277 - val_acc: 0.7192\n",
      "loss:  0.8402669099114014\n",
      "New learning rate:  0.002316985320603014\n",
      "Epoch 42/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.7992 - acc: 0.7425 - val_loss: 0.8185 - val_acc: 0.7127\n",
      "loss:  0.7992673128522322\n",
      "New learning rate:  0.002223910900480538\n",
      "Epoch 43/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.8206 - acc: 0.7382 - val_loss: 0.7807 - val_acc: 0.7249\n",
      "loss:  0.8197267419954124\n",
      "New learning rate:  0.0022698794900387043\n",
      "Epoch 44/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.8016 - acc: 0.7468 - val_loss: 0.7646 - val_acc: 0.7267\n",
      "loss:  0.8014079118624342\n",
      "New learning rate:  0.002228676500752767\n",
      "Epoch 45/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.8020 - acc: 0.7492 - val_loss: 0.7991 - val_acc: 0.7240\n",
      "loss:  0.8030006813035094\n",
      "New learning rate:  0.0022322290970595727\n",
      "Epoch 46/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.7840 - acc: 0.7481 - val_loss: 0.7575 - val_acc: 0.7380\n",
      "loss:  0.7831675047309515\n",
      "New learning rate:  0.002188393044232902\n",
      "Epoch 47/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.7800 - acc: 0.7555 - val_loss: 0.8548 - val_acc: 0.7218\n",
      "loss:  0.780381877232704\n",
      "New learning rate:  0.002182305479172711\n",
      "Epoch 48/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.7676 - acc: 0.7569 - val_loss: 0.7858 - val_acc: 0.7259\n",
      "loss:  0.7675568119243339\n",
      "New learning rate:  0.0021544959793848226\n",
      "Epoch 49/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.7577 - acc: 0.7515 - val_loss: 0.7189 - val_acc: 0.7433\n",
      "loss:  0.7585074812599449\n",
      "New learning rate:  0.002135087183606968\n",
      "Epoch 50/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.7373 - acc: 0.7628 - val_loss: 0.7292 - val_acc: 0.7453\n",
      "loss:  0.736869552931513\n",
      "New learning rate:  0.0020893845583677614\n",
      "Epoch 51/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.7375 - acc: 0.7654 - val_loss: 0.7618 - val_acc: 0.7448\n",
      "loss:  0.7367180552083594\n",
      "New learning rate:  0.002089068045340483\n",
      "Epoch 52/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.7529 - acc: 0.7586 - val_loss: 0.7349 - val_acc: 0.7475\n",
      "loss:  0.7529796757081766\n",
      "New learning rate:  0.00212331739733527\n",
      "Epoch 53/200\n",
      "189/189 [==============================] - 72s 381ms/step - loss: 0.7341 - acc: 0.7654 - val_loss: 0.7187 - val_acc: 0.7499\n",
      "loss:  0.7342574488652474\n",
      "New learning rate:  0.0020839339902915155\n",
      "Epoch 54/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.7351 - acc: 0.7666 - val_loss: 0.7125 - val_acc: 0.7429\n",
      "loss:  0.7355644306073825\n",
      "New learning rate:  0.0020866594346336713\n",
      "Epoch 55/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.7089 - acc: 0.7695 - val_loss: 0.7408 - val_acc: 0.7411\n",
      "loss:  0.7089151878836294\n",
      "New learning rate:  0.0020317859566702005\n",
      "Epoch 56/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.7120 - acc: 0.7773 - val_loss: 0.7123 - val_acc: 0.7397\n",
      "loss:  0.7121714441956027\n",
      "New learning rate:  0.0020384127559392195\n",
      "Epoch 57/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.7105 - acc: 0.7710 - val_loss: 0.7098 - val_acc: 0.7516\n",
      "loss:  0.711808372532676\n",
      "New learning rate:  0.0020376728003667676\n",
      "Epoch 58/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.7000 - acc: 0.7712 - val_loss: 0.6699 - val_acc: 0.7638\n",
      "loss:  0.7000204991726648\n",
      "New learning rate:  0.00201379398815804\n",
      "Epoch 59/200\n",
      "189/189 [==============================] - 72s 378ms/step - loss: 0.6952 - acc: 0.7771 - val_loss: 0.7237 - val_acc: 0.7469\n",
      "loss:  0.6951922416390717\n",
      "New learning rate:  0.0020040943072855305\n",
      "Epoch 60/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.6871 - acc: 0.7806 - val_loss: 0.6599 - val_acc: 0.7561\n",
      "loss:  0.6866271213038504\n",
      "New learning rate:  0.0019870023004191287\n",
      "Epoch 61/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.6875 - acc: 0.7794 - val_loss: 0.6464 - val_acc: 0.7778\n",
      "loss:  0.68762043464648\n",
      "New learning rate:  0.001988976996899787\n",
      "Epoch 62/200\n",
      "189/189 [==============================] - 72s 381ms/step - loss: 0.6705 - acc: 0.7846 - val_loss: 0.7101 - val_acc: 0.7529\n",
      "loss:  0.6713087390006428\n",
      "New learning rate:  0.001956796581570975\n",
      "Epoch 63/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.6531 - acc: 0.7962 - val_loss: 0.6727 - val_acc: 0.7645\n",
      "loss:  0.652524264439533\n",
      "New learning rate:  0.0019203822685771442\n",
      "Epoch 64/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.6643 - acc: 0.7894 - val_loss: 0.6645 - val_acc: 0.7607\n",
      "loss:  0.664269093303712\n",
      "New learning rate:  0.0019430697994647311\n",
      "Epoch 65/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.6901 - acc: 0.7779 - val_loss: 0.6828 - val_acc: 0.7604\n",
      "loss:  0.6898344121299308\n",
      "New learning rate:  0.001993385425466085\n",
      "Epoch 66/200\n",
      "189/189 [==============================] - 72s 380ms/step - loss: 0.6367 - acc: 0.7982 - val_loss: 0.6651 - val_acc: 0.7636\n",
      "loss:  0.6371531526115973\n",
      "New learning rate:  0.0018910895654316326\n",
      "Epoch 67/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.6472 - acc: 0.7864 - val_loss: 0.6549 - val_acc: 0.7781\n",
      "loss:  0.6477971769653279\n",
      "New learning rate:  0.0019113258756544614\n",
      "Epoch 68/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.6440 - acc: 0.7945 - val_loss: 0.6757 - val_acc: 0.7652\n",
      "loss:  0.644136132330764\n",
      "New learning rate:  0.0019043412196825499\n",
      "Epoch 69/200\n",
      "189/189 [==============================] - 72s 378ms/step - loss: 0.6275 - acc: 0.8011 - val_loss: 0.6586 - val_acc: 0.7722\n",
      "loss:  0.6274874172513447\n",
      "New learning rate:  0.0018728988490989468\n",
      "Epoch 70/200\n",
      "189/189 [==============================] - 72s 378ms/step - loss: 0.6718 - acc: 0.7885 - val_loss: 0.6512 - val_acc: 0.7584\n",
      "loss:  0.6712542177632337\n",
      "New learning rate:  0.00195668989750829\n",
      "Epoch 71/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.6367 - acc: 0.7960 - val_loss: 0.6936 - val_acc: 0.7656\n",
      "loss:  0.6364340752733574\n",
      "New learning rate:  0.0018897302145783191\n",
      "Epoch 72/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.6397 - acc: 0.7940 - val_loss: 0.6350 - val_acc: 0.7720\n",
      "loss:  0.6397408128730835\n",
      "New learning rate:  0.0018959893995699538\n",
      "Epoch 73/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.6285 - acc: 0.7923 - val_loss: 0.6525 - val_acc: 0.7625\n",
      "loss:  0.6278958266971516\n",
      "New learning rate:  0.001873663914899375\n",
      "Epoch 74/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.6276 - acc: 0.7963 - val_loss: 0.6339 - val_acc: 0.7712\n",
      "loss:  0.6276186660484031\n",
      "New learning rate:  0.0018731446809521176\n",
      "Epoch 75/200\n",
      "189/189 [==============================] - 72s 380ms/step - loss: 0.6283 - acc: 0.7955 - val_loss: 0.6675 - val_acc: 0.7707\n",
      "loss:  0.6291922083252605\n",
      "New learning rate:  0.001876094473501259\n",
      "Epoch 76/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.6138 - acc: 0.8046 - val_loss: 0.6872 - val_acc: 0.7640\n",
      "loss:  0.6140007422201478\n",
      "New learning rate:  0.001847809238959607\n",
      "Epoch 77/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.6205 - acc: 0.7952 - val_loss: 0.6039 - val_acc: 0.7856\n",
      "loss:  0.6202740395474059\n",
      "New learning rate:  0.0018594375314525517\n",
      "Epoch 78/200\n",
      "189/189 [==============================] - 72s 378ms/step - loss: 0.5903 - acc: 0.8079 - val_loss: 0.6202 - val_acc: 0.7710\n",
      "loss:  0.5912686745169347\n",
      "New learning rate:  0.0018062785419347366\n",
      "Epoch 79/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.5774 - acc: 0.8188 - val_loss: 0.6321 - val_acc: 0.7823\n",
      "loss:  0.5771659922506753\n",
      "New learning rate:  0.0017809839495990752\n",
      "Epoch 80/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.6096 - acc: 0.8071 - val_loss: 0.7056 - val_acc: 0.7698\n",
      "loss:  0.6098513737185098\n",
      "New learning rate:  0.0018401578826328464\n",
      "Epoch 81/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.5828 - acc: 0.8059 - val_loss: 0.6552 - val_acc: 0.7808\n",
      "loss:  0.5827548352104647\n",
      "New learning rate:  0.0017909654557613584\n",
      "Epoch 82/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.5947 - acc: 0.8072 - val_loss: 0.6528 - val_acc: 0.7713\n",
      "loss:  0.5948066047183385\n",
      "New learning rate:  0.0018126803472329608\n",
      "Epoch 83/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.5794 - acc: 0.8117 - val_loss: 0.6163 - val_acc: 0.7893\n",
      "loss:  0.5793324259064271\n",
      "New learning rate:  0.0017848465156536644\n",
      "Epoch 84/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.5660 - acc: 0.8198 - val_loss: 0.6467 - val_acc: 0.7699\n",
      "loss:  0.5660210454432422\n",
      "New learning rate:  0.0017612451763171022\n",
      "Epoch 85/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.5768 - acc: 0.8102 - val_loss: 0.6277 - val_acc: 0.7821\n",
      "loss:  0.5777176574749106\n",
      "New learning rate:  0.0017819667275662794\n",
      "Epoch 86/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.5894 - acc: 0.8091 - val_loss: 0.6090 - val_acc: 0.7917\n",
      "loss:  0.5895499259045416\n",
      "New learning rate:  0.001803176669630153\n",
      "Epoch 87/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.5460 - acc: 0.8231 - val_loss: 0.6083 - val_acc: 0.7864\n",
      "loss:  0.5459814437482723\n",
      "New learning rate:  0.001726301819362177\n",
      "Epoch 88/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.5724 - acc: 0.8195 - val_loss: 0.6070 - val_acc: 0.7817\n",
      "loss:  0.5731207841221022\n",
      "New learning rate:  0.0017737940509719317\n",
      "Epoch 89/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.5584 - acc: 0.8200 - val_loss: 0.6230 - val_acc: 0.7755\n",
      "loss:  0.5590169219508483\n",
      "New learning rate:  0.00174895229827477\n",
      "Epoch 90/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.5471 - acc: 0.8196 - val_loss: 0.6307 - val_acc: 0.7766\n",
      "loss:  0.5470973535504922\n",
      "New learning rate:  0.0017282292917266298\n",
      "Epoch 91/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.5563 - acc: 0.8220 - val_loss: 0.6271 - val_acc: 0.7797\n",
      "loss:  0.5558975996414096\n",
      "New learning rate:  0.0017435052523153318\n",
      "Epoch 92/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.5481 - acc: 0.8221 - val_loss: 0.6120 - val_acc: 0.7887\n",
      "loss:  0.5478253672236583\n",
      "New learning rate:  0.0017294879243766723\n",
      "Epoch 93/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.5681 - acc: 0.8161 - val_loss: 0.6295 - val_acc: 0.7784\n",
      "loss:  0.5687559611706469\n",
      "New learning rate:  0.0017660686263077255\n",
      "Epoch 94/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.5492 - acc: 0.8187 - val_loss: 0.6641 - val_acc: 0.7810\n",
      "loss:  0.5497480425394164\n",
      "New learning rate:  0.0017328163668495155\n",
      "Epoch 95/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.5610 - acc: 0.8125 - val_loss: 0.5690 - val_acc: 0.8020\n",
      "loss:  0.5604704050061716\n",
      "New learning rate:  0.0017514962191299496\n",
      "Epoch 96/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.5385 - acc: 0.8250 - val_loss: 0.6655 - val_acc: 0.7755\n",
      "loss:  0.5388100122753607\n",
      "New learning rate:  0.0017139660495952547\n",
      "Epoch 97/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.5550 - acc: 0.8226 - val_loss: 0.6098 - val_acc: 0.7935\n",
      "loss:  0.5549548347160299\n",
      "New learning rate:  0.0017418623112915076\n",
      "Epoch 98/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.5350 - acc: 0.8287 - val_loss: 0.5911 - val_acc: 0.7899\n",
      "loss:  0.5347903275640254\n",
      "New learning rate:  0.0017070902749512073\n",
      "Epoch 99/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.5354 - acc: 0.8224 - val_loss: 0.6242 - val_acc: 0.7920\n",
      "loss:  0.5353703073881291\n",
      "New learning rate:  0.0017080806400367253\n",
      "Epoch 100/200\n",
      "189/189 [==============================] - 71s 374ms/step - loss: 0.5531 - acc: 0.8251 - val_loss: 0.6464 - val_acc: 0.7814\n",
      "loss:  0.5536103438323652\n",
      "New learning rate:  0.0017395219669317497\n",
      "Epoch 101/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.5234 - acc: 0.8233 - val_loss: 0.5788 - val_acc: 0.7955\n",
      "loss:  0.5236216706716511\n",
      "New learning rate:  0.0016881304443923787\n",
      "Epoch 102/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.5027 - acc: 0.8335 - val_loss: 0.6351 - val_acc: 0.7878\n",
      "loss:  0.5026694272087995\n",
      "New learning rate:  0.0016531282916136523\n",
      "Epoch 103/200\n",
      "189/189 [==============================] - 71s 373ms/step - loss: 0.5233 - acc: 0.8263 - val_loss: 0.6144 - val_acc: 0.7930\n",
      "loss:  0.5223708189314026\n",
      "New learning rate:  0.0016860201635876034\n",
      "Epoch 104/200\n",
      "189/189 [==============================] - 71s 374ms/step - loss: 0.5228 - acc: 0.8269 - val_loss: 0.5553 - val_acc: 0.8016\n",
      "loss:  0.5223483268365794\n",
      "New learning rate:  0.0016859822418886807\n",
      "Epoch 105/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.5187 - acc: 0.8294 - val_loss: 0.5526 - val_acc: 0.7950\n",
      "loss:  0.5186601172363947\n",
      "New learning rate:  0.0016797754390292224\n",
      "Epoch 106/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.5244 - acc: 0.8329 - val_loss: 0.5960 - val_acc: 0.7948\n",
      "loss:  0.5243137754580158\n",
      "New learning rate:  0.0016892992119611108\n",
      "Epoch 107/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.5011 - acc: 0.8356 - val_loss: 0.6652 - val_acc: 0.7755\n",
      "loss:  0.5018102009206941\n",
      "New learning rate:  0.0016517084903805083\n",
      "Epoch 108/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4983 - acc: 0.8363 - val_loss: 0.6378 - val_acc: 0.7816\n",
      "loss:  0.4982995832407916\n",
      "New learning rate:  0.0016459201396402023\n",
      "Epoch 109/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.5146 - acc: 0.8302 - val_loss: 0.6218 - val_acc: 0.7918\n",
      "loss:  0.5140644959815908\n",
      "New learning rate:  0.0016720735383826488\n",
      "Epoch 110/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.5012 - acc: 0.8415 - val_loss: 0.5993 - val_acc: 0.7898\n",
      "loss:  0.5001044422906593\n",
      "New learning rate:  0.001648893475918896\n",
      "Epoch 111/200\n",
      "189/189 [==============================] - 71s 374ms/step - loss: 0.4984 - acc: 0.8386 - val_loss: 0.5657 - val_acc: 0.7959\n",
      "loss:  0.4984342513592063\n",
      "New learning rate:  0.0016461418075339328\n",
      "Epoch 112/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.5046 - acc: 0.8337 - val_loss: 0.6595 - val_acc: 0.7731\n",
      "loss:  0.5046105324906647\n",
      "New learning rate:  0.0016563403040907544\n",
      "Epoch 113/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.5049 - acc: 0.8374 - val_loss: 0.5847 - val_acc: 0.7909\n",
      "loss:  0.5053710605155958\n",
      "New learning rate:  0.0016576004764482948\n",
      "Epoch 114/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.4807 - acc: 0.8482 - val_loss: 0.6278 - val_acc: 0.7918\n",
      "loss:  0.47969004477190835\n",
      "New learning rate:  0.0016155735691048753\n",
      "Epoch 115/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.4918 - acc: 0.8386 - val_loss: 0.6450 - val_acc: 0.7881\n",
      "loss:  0.4918415899629946\n",
      "New learning rate:  0.001635325046784769\n",
      "Epoch 116/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.4676 - acc: 0.8451 - val_loss: 0.6652 - val_acc: 0.7736\n",
      "loss:  0.4674756720921375\n",
      "New learning rate:  0.0015959603767028393\n",
      "Epoch 117/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4897 - acc: 0.8428 - val_loss: 0.5993 - val_acc: 0.7893\n",
      "loss:  0.4892959308011925\n",
      "New learning rate:  0.001631167360867802\n",
      "Epoch 118/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4808 - acc: 0.8467 - val_loss: 0.5996 - val_acc: 0.7899\n",
      "loss:  0.4803209566705768\n",
      "New learning rate:  0.001616593175299817\n",
      "Epoch 119/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.5028 - acc: 0.8358 - val_loss: 0.5905 - val_acc: 0.8014\n",
      "loss:  0.5028233627478281\n",
      "New learning rate:  0.0016533827863956985\n",
      "Epoch 120/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.4737 - acc: 0.8413 - val_loss: 0.6454 - val_acc: 0.7767\n",
      "loss:  0.4743150179975766\n",
      "New learning rate:  0.0016069131138551512\n",
      "Epoch 121/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.4701 - acc: 0.8484 - val_loss: 0.6098 - val_acc: 0.7972\n",
      "loss:  0.4701590498272012\n",
      "New learning rate:  0.001600248692255792\n",
      "Epoch 122/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4768 - acc: 0.8461 - val_loss: 0.6380 - val_acc: 0.7897\n",
      "loss:  0.47671192706174464\n",
      "New learning rate:  0.001610769358150301\n",
      "Epoch 123/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.4688 - acc: 0.8447 - val_loss: 0.6056 - val_acc: 0.7995\n",
      "loss:  0.4688185398600372\n",
      "New learning rate:  0.0015981049800886603\n",
      "Epoch 124/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.4618 - acc: 0.8469 - val_loss: 0.5866 - val_acc: 0.8010\n",
      "loss:  0.46154885181436484\n",
      "New learning rate:  0.0015865293818923358\n",
      "Epoch 125/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.4638 - acc: 0.8479 - val_loss: 0.5937 - val_acc: 0.8070\n",
      "loss:  0.4636533460739534\n",
      "New learning rate:  0.0015898717396219451\n",
      "Epoch 126/200\n",
      "189/189 [==============================] - 71s 374ms/step - loss: 0.4647 - acc: 0.8498 - val_loss: 0.6038 - val_acc: 0.7968\n",
      "loss:  0.4645788529142429\n",
      "New learning rate:  0.001591343857914843\n",
      "Epoch 127/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.4496 - acc: 0.8561 - val_loss: 0.5963 - val_acc: 0.7896\n",
      "loss:  0.4479930676799484\n",
      "New learning rate:  0.0015651678453714743\n",
      "Epoch 128/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.4661 - acc: 0.8429 - val_loss: 0.6362 - val_acc: 0.7881\n",
      "loss:  0.46613969847008035\n",
      "New learning rate:  0.0015938296393590754\n",
      "Epoch 129/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.4765 - acc: 0.8454 - val_loss: 0.5946 - val_acc: 0.7915\n",
      "loss:  0.47590696067975985\n",
      "New learning rate:  0.0016094732646936751\n",
      "Epoch 130/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.4445 - acc: 0.8597 - val_loss: 0.6016 - val_acc: 0.7873\n",
      "loss:  0.4441285770593247\n",
      "New learning rate:  0.0015591309412061668\n",
      "Epoch 131/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.4315 - acc: 0.8567 - val_loss: 0.5963 - val_acc: 0.8048\n",
      "loss:  0.430314162232004\n",
      "New learning rate:  0.00153774054767304\n",
      "Epoch 132/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4432 - acc: 0.8520 - val_loss: 0.6441 - val_acc: 0.7824\n",
      "loss:  0.44323705081586484\n",
      "New learning rate:  0.0015577415544844795\n",
      "Epoch 133/200\n",
      "189/189 [==============================] - 71s 374ms/step - loss: 0.4602 - acc: 0.8521 - val_loss: 0.5839 - val_acc: 0.7993\n",
      "loss:  0.4606163361542661\n",
      "New learning rate:  0.0015850506079955829\n",
      "Epoch 134/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4329 - acc: 0.8608 - val_loss: 0.5535 - val_acc: 0.8086\n",
      "loss:  0.43237571332112956\n",
      "New learning rate:  0.001540913948313751\n",
      "Epoch 135/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4663 - acc: 0.8535 - val_loss: 0.5733 - val_acc: 0.8049\n",
      "loss:  0.4652018174999222\n",
      "New learning rate:  0.0015923355176343345\n",
      "Epoch 136/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.4234 - acc: 0.8611 - val_loss: 0.5598 - val_acc: 0.7983\n",
      "loss:  0.42338026311031723\n",
      "New learning rate:  0.0015271148910282926\n",
      "Epoch 137/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.4445 - acc: 0.8552 - val_loss: 0.5500 - val_acc: 0.7979\n",
      "loss:  0.4444149038173388\n",
      "New learning rate:  0.0015595774260311077\n",
      "Epoch 138/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4426 - acc: 0.8584 - val_loss: 0.6189 - val_acc: 0.7972\n",
      "loss:  0.44214293458203624\n",
      "New learning rate:  0.0015560381362003362\n",
      "Epoch 139/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.4440 - acc: 0.8616 - val_loss: 0.5854 - val_acc: 0.7977\n",
      "loss:  0.44401015983845193\n",
      "New learning rate:  0.0015589463241842654\n",
      "Epoch 140/200\n",
      "189/189 [==============================] - 71s 374ms/step - loss: 0.4517 - acc: 0.8515 - val_loss: 0.5865 - val_acc: 0.7929\n",
      "loss:  0.4517259938758063\n",
      "New learning rate:  0.0015710214201062752\n",
      "Epoch 141/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4314 - acc: 0.8580 - val_loss: 0.5862 - val_acc: 0.7961\n",
      "loss:  0.4314251239337618\n",
      "New learning rate:  0.0015394498678475612\n",
      "Epoch 142/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4482 - acc: 0.8575 - val_loss: 0.5909 - val_acc: 0.7996\n",
      "loss:  0.4480098976413376\n",
      "New learning rate:  0.0015651941873075458\n",
      "Epoch 143/200\n",
      "189/189 [==============================] - 75s 397ms/step - loss: 0.4169 - acc: 0.8604 - val_loss: 0.6016 - val_acc: 0.7987\n",
      "loss:  0.416504539883402\n",
      "New learning rate:  0.0015166508867248804\n",
      "Epoch 144/200\n",
      "189/189 [==============================] - 70s 370ms/step - loss: 0.4178 - acc: 0.8611 - val_loss: 0.5762 - val_acc: 0.8064\n",
      "loss:  0.4177920373265075\n",
      "New learning rate:  0.0015186048284411917\n",
      "Epoch 145/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.4126 - acc: 0.8657 - val_loss: 0.5825 - val_acc: 0.8051\n",
      "loss:  0.4127544157398369\n",
      "New learning rate:  0.001510973908956058\n",
      "Epoch 146/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4222 - acc: 0.8584 - val_loss: 0.5906 - val_acc: 0.7866\n",
      "loss:  0.4226394800233169\n",
      "New learning rate:  0.0015259840490511314\n",
      "Epoch 147/200\n",
      "189/189 [==============================] - 72s 378ms/step - loss: 0.4162 - acc: 0.8604 - val_loss: 0.5985 - val_acc: 0.7953\n",
      "loss:  0.4164420360509482\n",
      "New learning rate:  0.001516556093194476\n",
      "Epoch 148/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.4123 - acc: 0.8642 - val_loss: 0.5694 - val_acc: 0.8009\n",
      "loss:  0.4121365925943743\n",
      "New learning rate:  0.001510040682617016\n",
      "Epoch 149/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.4218 - acc: 0.8638 - val_loss: 0.5713 - val_acc: 0.8031\n",
      "loss:  0.4211316812996442\n",
      "New learning rate:  0.0015236849060076838\n",
      "Epoch 150/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4098 - acc: 0.8584 - val_loss: 0.6057 - val_acc: 0.7951\n",
      "loss:  0.40998321340512717\n",
      "New learning rate:  0.0015067924909854496\n",
      "Epoch 151/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.4095 - acc: 0.8669 - val_loss: 0.6394 - val_acc: 0.7887\n",
      "loss:  0.409720311237942\n",
      "New learning rate:  0.0015063964040424103\n",
      "Epoch 152/200\n",
      "189/189 [==============================] - 72s 378ms/step - loss: 0.4180 - acc: 0.8639 - val_loss: 0.6131 - val_acc: 0.8034\n",
      "loss:  0.41826927966091787\n",
      "New learning rate:  0.001519329743920983\n",
      "Epoch 153/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.4074 - acc: 0.8659 - val_loss: 0.5915 - val_acc: 0.7984\n",
      "loss:  0.40710752381701654\n",
      "New learning rate:  0.0015024656478183698\n",
      "Epoch 154/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4167 - acc: 0.8616 - val_loss: 0.5897 - val_acc: 0.7922\n",
      "loss:  0.4162713001558577\n",
      "New learning rate:  0.0015162971847355621\n",
      "Epoch 155/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.4223 - acc: 0.8626 - val_loss: 0.6470 - val_acc: 0.7878\n",
      "loss:  0.42224477951252076\n",
      "New learning rate:  0.0015253818612172448\n",
      "Epoch 156/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.3967 - acc: 0.8668 - val_loss: 0.5593 - val_acc: 0.8094\n",
      "loss:  0.39601348637546896\n",
      "New learning rate:  0.0014858893566780313\n",
      "Epoch 157/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.4026 - acc: 0.8662 - val_loss: 0.5404 - val_acc: 0.8108\n",
      "loss:  0.40244206381335046\n",
      "New learning rate:  0.0014954722807538551\n",
      "Epoch 158/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.3930 - acc: 0.8733 - val_loss: 0.5699 - val_acc: 0.7978\n",
      "loss:  0.39299773996469206\n",
      "New learning rate:  0.001481415041275182\n",
      "Epoch 159/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.3832 - acc: 0.8731 - val_loss: 0.6111 - val_acc: 0.8007\n",
      "loss:  0.3837061894685647\n",
      "New learning rate:  0.0014677141484518937\n",
      "Epoch 160/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.3961 - acc: 0.8624 - val_loss: 0.5760 - val_acc: 0.8012\n",
      "loss:  0.39576116062534084\n",
      "New learning rate:  0.0014855144758295347\n",
      "Epoch 161/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.3956 - acc: 0.8689 - val_loss: 0.5769 - val_acc: 0.8047\n",
      "loss:  0.39596040786152154\n",
      "New learning rate:  0.0014858104899721706\n",
      "Epoch 162/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.3993 - acc: 0.8716 - val_loss: 0.5883 - val_acc: 0.7999\n",
      "loss:  0.3995277903399985\n",
      "New learning rate:  0.0014911204099068381\n",
      "Epoch 163/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.3911 - acc: 0.8712 - val_loss: 0.5751 - val_acc: 0.8076\n",
      "loss:  0.3913619114923991\n",
      "New learning rate:  0.0014789936813760635\n",
      "Epoch 164/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.3885 - acc: 0.8726 - val_loss: 0.6781 - val_acc: 0.7767\n",
      "loss:  0.38880112590924115\n",
      "New learning rate:  0.0014752111408838334\n",
      "Epoch 165/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.3880 - acc: 0.8688 - val_loss: 0.6769 - val_acc: 0.7847\n",
      "loss:  0.3884530976130731\n",
      "New learning rate:  0.001474697814995132\n",
      "Epoch 166/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.3854 - acc: 0.8714 - val_loss: 0.5860 - val_acc: 0.8066\n",
      "loss:  0.38513844951481097\n",
      "New learning rate:  0.001469817802916538\n",
      "Epoch 167/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.3905 - acc: 0.8721 - val_loss: 0.5658 - val_acc: 0.8105\n",
      "loss:  0.39010805888124606\n",
      "New learning rate:  0.001477140403398309\n",
      "Epoch 168/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.3805 - acc: 0.8760 - val_loss: 0.5727 - val_acc: 0.8122\n",
      "loss:  0.3809267529225488\n",
      "New learning rate:  0.0014636403941020117\n",
      "Epoch 169/200\n",
      "189/189 [==============================] - 71s 374ms/step - loss: 0.3829 - acc: 0.8700 - val_loss: 0.5874 - val_acc: 0.7990\n",
      "loss:  0.3831573334328873\n",
      "New learning rate:  0.0014669088057116958\n",
      "Epoch 170/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.3766 - acc: 0.8732 - val_loss: 0.5922 - val_acc: 0.8063\n",
      "loss:  0.3763502284378282\n",
      "New learning rate:  0.0014569573123098276\n",
      "Epoch 171/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.3881 - acc: 0.8705 - val_loss: 0.5765 - val_acc: 0.7948\n",
      "loss:  0.388461677849836\n",
      "New learning rate:  0.0014747104683058222\n",
      "Epoch 172/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.3947 - acc: 0.8647 - val_loss: 0.5929 - val_acc: 0.8021\n",
      "loss:  0.3948554582998786\n",
      "New learning rate:  0.0014841696510116801\n",
      "Epoch 173/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.3725 - acc: 0.8787 - val_loss: 0.6107 - val_acc: 0.8053\n",
      "loss:  0.3731319969714836\n",
      "New learning rate:  0.0014522760232185067\n",
      "Epoch 174/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.3658 - acc: 0.8796 - val_loss: 0.5682 - val_acc: 0.8028\n",
      "loss:  0.36583695623767437\n",
      "New learning rate:  0.0014417201600115486\n",
      "Epoch 175/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.3592 - acc: 0.8825 - val_loss: 0.5933 - val_acc: 0.8072\n",
      "loss:  0.35909864176573986\n",
      "New learning rate:  0.0014320380533670717\n",
      "Epoch 176/200\n",
      "189/189 [==============================] - 71s 374ms/step - loss: 0.3682 - acc: 0.8777 - val_loss: 0.5637 - val_acc: 0.8154\n",
      "loss:  0.36820532925482513\n",
      "New learning rate:  0.001445138737772511\n",
      "Epoch 177/200\n",
      "189/189 [==============================] - 71s 374ms/step - loss: 0.3768 - acc: 0.8755 - val_loss: 0.5989 - val_acc: 0.8068\n",
      "loss:  0.3766738920490309\n",
      "New learning rate:  0.001457428952697196\n",
      "Epoch 178/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.3691 - acc: 0.8733 - val_loss: 0.5496 - val_acc: 0.8045\n",
      "loss:  0.3686460053614575\n",
      "New learning rate:  0.0014457757162253\n",
      "Epoch 179/200\n",
      "189/189 [==============================] - 71s 374ms/step - loss: 0.3690 - acc: 0.8795 - val_loss: 0.5403 - val_acc: 0.8160\n",
      "loss:  0.36841916217227166\n",
      "New learning rate:  0.001445447789046414\n",
      "Epoch 180/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.3490 - acc: 0.8822 - val_loss: 0.5635 - val_acc: 0.8071\n",
      "loss:  0.3492411772056167\n",
      "New learning rate:  0.0014179911362453789\n",
      "Epoch 181/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.3478 - acc: 0.8836 - val_loss: 0.6413 - val_acc: 0.7920\n",
      "loss:  0.3470341702977191\n",
      "New learning rate:  0.0014148650709051385\n",
      "Epoch 182/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.3892 - acc: 0.8695 - val_loss: 0.5822 - val_acc: 0.7962\n",
      "loss:  0.38919479031155896\n",
      "New learning rate:  0.001475791993318864\n",
      "Epoch 183/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.3732 - acc: 0.8723 - val_loss: 0.5759 - val_acc: 0.7943\n",
      "loss:  0.37324516651742995\n",
      "New learning rate:  0.0014524403859368948\n",
      "Epoch 184/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.3529 - acc: 0.8798 - val_loss: 0.6099 - val_acc: 0.7908\n",
      "loss:  0.35290974174142375\n",
      "New learning rate:  0.0014232026818406655\n",
      "Epoch 185/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.3604 - acc: 0.8847 - val_loss: 0.5547 - val_acc: 0.8136\n",
      "loss:  0.3600461266015123\n",
      "New learning rate:  0.0014333955306999261\n",
      "Epoch 186/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.3551 - acc: 0.8804 - val_loss: 0.6199 - val_acc: 0.8035\n",
      "loss:  0.3550000737398838\n",
      "New learning rate:  0.0014261807594478798\n",
      "Epoch 187/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.3571 - acc: 0.8857 - val_loss: 0.6021 - val_acc: 0.8028\n",
      "loss:  0.3573285419114286\n",
      "New learning rate:  0.00142950544516342\n",
      "Epoch 188/200\n",
      "189/189 [==============================] - 71s 376ms/step - loss: 0.3438 - acc: 0.8856 - val_loss: 0.5589 - val_acc: 0.8059\n",
      "loss:  0.34310474008593367\n",
      "New learning rate:  0.001409316366113573\n",
      "Epoch 189/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.3736 - acc: 0.8772 - val_loss: 0.6419 - val_acc: 0.8063\n",
      "loss:  0.37357338469879825\n",
      "New learning rate:  0.0014529171815211223\n",
      "Epoch 190/200\n",
      "189/189 [==============================] - 71s 375ms/step - loss: 0.3407 - acc: 0.8870 - val_loss: 0.5575 - val_acc: 0.8119\n",
      "loss:  0.3410871428330278\n",
      "New learning rate:  0.001406475799807805\n",
      "Epoch 191/200\n",
      "189/189 [==============================] - 70s 371ms/step - loss: 0.3488 - acc: 0.8867 - val_loss: 0.5579 - val_acc: 0.8040\n",
      "loss:  0.3489732807790729\n",
      "New learning rate:  0.0014176113123661096\n",
      "Epoch 192/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.3369 - acc: 0.8877 - val_loss: 0.5565 - val_acc: 0.8047\n",
      "loss:  0.3374854315013656\n",
      "New learning rate:  0.001401419191669673\n",
      "Epoch 193/200\n",
      "189/189 [==============================] - 72s 380ms/step - loss: 0.3528 - acc: 0.8840 - val_loss: 0.6279 - val_acc: 0.8017\n",
      "loss:  0.3526865072752092\n",
      "New learning rate:  0.0014228850094087067\n",
      "Epoch 194/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.3529 - acc: 0.8836 - val_loss: 0.6042 - val_acc: 0.8019\n",
      "loss:  0.3533412978756793\n",
      "New learning rate:  0.001423817006236499\n",
      "Epoch 195/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.3487 - acc: 0.8831 - val_loss: 0.5897 - val_acc: 0.8095\n",
      "loss:  0.3486685309315281\n",
      "New learning rate:  0.0014171793613566334\n",
      "Epoch 196/200\n",
      "189/189 [==============================] - 71s 377ms/step - loss: 0.3349 - acc: 0.8899 - val_loss: 0.6201 - val_acc: 0.8012\n",
      "loss:  0.3349268609096134\n",
      "New learning rate:  0.0013978381448732059\n",
      "Epoch 197/200\n",
      "189/189 [==============================] - 72s 380ms/step - loss: 0.3450 - acc: 0.8874 - val_loss: 0.5669 - val_acc: 0.8157\n",
      "loss:  0.34516551262692574\n",
      "New learning rate:  0.0014122236411698524\n",
      "Epoch 198/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.3427 - acc: 0.8854 - val_loss: 0.5989 - val_acc: 0.8046\n",
      "loss:  0.3429385232401961\n",
      "New learning rate:  0.0014090821334597967\n",
      "Epoch 199/200\n",
      "189/189 [==============================] - 71s 378ms/step - loss: 0.3439 - acc: 0.8840 - val_loss: 0.6010 - val_acc: 0.8096\n",
      "loss:  0.3443202695327054\n",
      "New learning rate:  0.001411030473219181\n",
      "Epoch 200/200\n",
      "189/189 [==============================] - 72s 379ms/step - loss: 0.3510 - acc: 0.8846 - val_loss: 0.5788 - val_acc: 0.8097\n",
      "loss:  0.3501409766292809\n",
      "New learning rate:  0.0014192676180552078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `evaluate_generator` call to the Keras 2 API: `evaluate_generator(<keras_pre..., steps=1617)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8082840929035278\n"
     ]
    }
   ],
   "source": [
    "#epochs = 100\n",
    "epochs = 200\n",
    "#epochs = 400\n",
    "#epochs = 600\n",
    "samples_per_epoch = 6067\n",
    "val_samples = 1617\n",
    "\n",
    "#Fit the model\n",
    "hist = History()\n",
    "model.fit_generator(train_generator,\n",
    "                    samples_per_epoch= samples_per_epoch,\n",
    "                    nb_epoch=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=test_generator,\n",
    "                    nb_val_samples=val_samples, \n",
    "                    callbacks = [lrate, hist, mc])\n",
    "\n",
    "#evaluate the model\n",
    "scores = model.evaluate_generator(test_generator, val_samples=val_samples) \n",
    "print(\"Accuracy = \", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "mDQchgLItYaE",
    "outputId": "f1881550-8883-4eac-e58b-b414111bfe63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., verbose=1, validation_data=<keras_pre..., callbacks=[<__main__..., steps_per_epoch=189, epochs=100, validation_steps=1617)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 53/189 [=======>......................] - ETA: 22s - loss: 0.3308 - acc: 0.8897"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-80ffac6df984>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                     callbacks = [lrate, hist, mc])\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###COmpare\n",
    "epochs = 100\n",
    "#epochs = 200\n",
    "samples_per_epoch =  6067\n",
    "val_samples = 1617\n",
    "\n",
    "#Fit the model\n",
    "hist = History()\n",
    "model.fit_generator(train_generator,\n",
    "                    samples_per_epoch= samples_per_epoch,\n",
    "                    nb_epoch=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=test_generator,\n",
    "                    nb_val_samples=val_samples, \n",
    "                    callbacks = [lrate, hist, mc])\n",
    "\n",
    "#evaluate the model\n",
    "scores = model.evaluate_generator(test_generator, val_samples=val_samples) \n",
    "print(\"Accuracy = \", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "SeJ8HSi1CFdN",
    "outputId": "843c7c45-d2c2-4221-c6f2-af49fe580148"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0UCEEELI6/+wKL6+8KDCvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(hist.history['val_loss'], label='Test Data')\n",
    "plt.plot(hist.history['loss'], label='Training Data')\n",
    "plt.legend()\n",
    "plt.savefig('e200_m3_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0S9F7hk2rI9"
   },
   "outputs": [],
   "source": [
    "!find . -name \"*\\._*\" -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sp_Vf5lW3A-C",
    "outputId": "e39f7e08-285a-4a01-f1e0-a97f60f69abe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find: test1/Apple: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!find  'test1/Apple' -type f -name '\\._*'  -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "2PP3yDyR3kCZ",
    "outputId": "f86955f5-9bf2-4756-88c8-477a46692ef2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "LHN3KvbY3mCH",
    "outputId": "91614a0a-56f7-405c-e47b-2834c026ac40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Accuracy Graph.png'   outFolder.tar.gz\t\t weights00000025.h5\n",
      "'Accuracy Loss.png'    sample_data\t\t weights00000050.h5\n",
      " Accuracy.png\t       Validation_accuracy.png\t weights00000075.h5\n",
      " Loss.png\t       Validation_Accuracy.png\t weights00000100.h5\n",
      " newData\t       Validation_loss.png\n",
      " newData.tar.gz        Validation_Loss.png\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DwSXL2S87q-U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "colab_type": "code",
    "id": "rzPnFzft7xHQ",
    "outputId": "264e872b-2f89-4193-b766-87f73d9602c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-bf65f38a3a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         shuffle=False)  # keep data in same order as labels\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m210\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1770\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1772\u001b[0;31m             verbose=verbose)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(model, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;31m# Compatibility with the generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generator = test_datagen.flow_from_directory(\n",
    "        'Dataset/prediction_generator/Apple',\n",
    "        target_size=(128, 128),\n",
    "        batch_size=1,\n",
    "        class_mode=None,  # only data, no labels\n",
    "        shuffle=False)  # keep data in same order as labels\n",
    "\n",
    "probabilities = model.predict_generator(generator, 210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jb5TpnhTzWmb"
   },
   "outputs": [],
   "source": [
    "l=[i.argmax() for i in probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9h22UqDz0NMC",
    "outputId": "919a08f2-e3f4-40d9-da3e-a000aec88e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in l:\n",
    "  if(i!=0):\n",
    "    count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hQreuOkD4VhF",
    "outputId": "8501221e-3daf-46be-a5ce-1fabd93714df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5]"
      ]
     },
     "execution_count": 141,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXbGb6x54wUX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ml_test_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
